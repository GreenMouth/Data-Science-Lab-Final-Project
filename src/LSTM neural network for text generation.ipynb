{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/corbosiny/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##First we'll import all our tools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17394\n"
     ]
    }
   ],
   "source": [
    "##Let's get our scripts ready to go, we'll read in all the \n",
    "def getData(csvName= 'clean_transcripts.csv'):\n",
    "    dataFrame = pd.read_csv(csvName)\n",
    "    allScripts = dataFrame['transcript'].tolist()\n",
    "    allScripts = [script.lower() for script in allScripts]\n",
    "    return allScripts\n",
    "\n",
    "\n",
    "useAll = False #Set to true to use all transcripts in your training data, false uses the first transcript\n",
    "\n",
    "allScripts = getData()\n",
    "transcript = ''\n",
    "\n",
    "if not useAll:\n",
    "    transcript = allScripts[0]\n",
    "else:\n",
    "    transcript = \"\\n\".join(allScripts) #Joining our transcripts seperated by new lines\n",
    "\n",
    "print(len(transcript))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '5', '6', '9', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "##Let's look at a list of all unique characters in our scripts, \n",
    "##we'll eventually need to one hot encode them to make training easier:\n",
    "uniqueChars = sorted(list(set(transcript)))\n",
    "numUniqueChars = len(uniqueChars)\n",
    "print(uniqueChars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'?': 18, '2': 11, 'c': 21, 'h': 26, 'y': 43, 'w': 41, 'r': 36, '\"': 2, 's': 37, 'u': 39, 'a': 19, 'f': 24, 'p': 34, 'g': 25, 'd': 22, 'b': 20, ',': 6, 't': 38, '.': 8, 'o': 33, 'k': 29, 'i': 27, 'm': 31, '(': 4, ' ': 0, '0': 9, 'j': 28, ':': 16, '-': 7, 'e': 23, '5': 13, '1': 10, '3': 12, 'z': 44, 'q': 35, '6': 14, 'v': 40, '!': 1, 'l': 30, 'n': 32, ';': 17, \"'\": 3, ')': 5, 'x': 42, '9': 15}\n"
     ]
    }
   ],
   "source": [
    "##Lets make a mapping of each character to a specific number, this will help our training since we need numerical data:\n",
    "charsToInt = dict((char, i) for i, char in enumerate(uniqueChars))\n",
    "print(charsToInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 33, 33, 22, 0, 31, 33, 36, 32, 27, 32, 25, 8, 0, 26, 33, 41, 0, 19, 36, 23, 0, 43, 33, 39, 18, 4, 30, 19, 39, 25, 26, 38, 23, 36, 5, 27, 38, 3, 37, 0, 20, 23, 23, 32, 0, 25, 36, 23, 19, 38, 6, 0, 26, 19, 37, 32, 3, 38, 0, 27, 38, 18, 0, 27, 3, 40, 23, 0, 20, 23, 23, 32, 0, 20, 30, 33, 41, 32, 0, 19, 41, 19, 43, 0, 20, 43, 0, 38, 26, 23, 0, 41, 26, 33, 30, 23, 0, 38, 26]\n"
     ]
    }
   ],
   "source": [
    "##For our network we are going to train it by feeding it strings of characters and have it predict what the next\n",
    "##character in the sequence will be. So to generate these sequences we will copy 100 characters from our transcript\n",
    "##into a sequence, take the next character as the target answer, and then shift our window by one character and do that\n",
    "##over and over again. \n",
    "\n",
    "lengthOfSequence = 100\n",
    "def prepSequences(rawText, encoding, sequenceLength = 100): \n",
    "    data = []\n",
    "    targets = []\n",
    "    for i in range(0, len(rawText) - sequenceLength, 1):\n",
    "        sequence = rawText[i: i+sequenceLength]\n",
    "        target = rawText[i + sequenceLength]\n",
    "        data.append([encoding[char] for char in sequence]) #Here we are encoding the characters to their previous assigned values\n",
    "        targets.append(encoding[target])                   #Same with the target answer\n",
    "\n",
    "    return data, targets\n",
    "\n",
    "data, targets = prepSequences(transcript, charsToInt, lengthOfSequence)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##To finish off prepping our data, we need to convert x to be [samples, time steps, features]\n",
    "##and we need to convert our training answers to a one hot encoding\n",
    "def prepX(data, lengthOfSequence, numUniqueChars):\n",
    "    data = np.reshape(data, (len(data), lengthOfSequence, 1))\n",
    "    data = data / float(numUniqueChars)\n",
    "    return data\n",
    "\n",
    "def prepY(targets):\n",
    "    targets = np_utils.to_categorical(targets)\n",
    "    return targets\n",
    "\n",
    "preppedX = prepX(data, lengthOfSequence, numUniqueChars)\n",
    "preppedY = prepY(targets)\n",
    "\n",
    "\n",
    "##The last thing we can do before we train is get our model set up\n",
    "def generateModel(X, y, size= 256):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(size, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(size))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y.shape[1], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "model = generateModel(preppedX, preppedY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 117s 7ms/step - loss: 3.0563\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.05627, saving model to weights-improvement-01-3.0563.hdf5\n"
     ]
    }
   ],
   "source": [
    "##Training time!\n",
    "\n",
    "def trainModel(model, X, y, numEpochs= 20, batchSize= 128):\n",
    "    filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\" #replace with lowest loss file\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    model.fit(X, y, epochs = numEpochs, batch_size= batchSize, callbacks=callbacks_list)\n",
    "    return model\n",
    "\n",
    "model = trainModel(model, preppedX, preppedY, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##The code below loads back in the best weights we add\n",
    "def loadModel(model, filename):  #replace with best weights file for your training\n",
    "    model.load_weights(filename)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "model = loadModel(model, \"weights-improvement-01-3.0563.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Seed:  oo, there's a hierarchy within the arts. art and music are normally given a higher status in schools\n",
      "\n",
      "\n",
      "oo, there's a hierarchy within the arts. art and music are normally given a higher status in schools                                                                                                                                                                                                        \n"
     ]
    }
   ],
   "source": [
    "##Now for text generation\n",
    "def generateSeedFromData(data):\n",
    "    start = np.random.randint(0, len(data)-1)\n",
    "    pattern = data[start]\n",
    "    return pattern\n",
    "\n",
    "\n",
    "\n",
    "def generateText(model, pattern, decoding, length= 1000, vocabSize= 47, delimeter= ''):\n",
    "    text= delimeter.join([decoding[value] for value in pattern])\n",
    "    for i in range(length):\n",
    "        preppedPattern = prepPattern(pattern, vocabSize)\n",
    "        prediction = model.predict(preppedPattern, verbose= 0)\n",
    "        index = np.argmax(prediction)\n",
    "        result = decoding[index]\n",
    "        text += delimeter + result\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:]\n",
    "        \n",
    "    return text\n",
    "\n",
    "def prepPattern(pattern, vocabSize):\n",
    "    pattern = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    pattern = pattern / float(vocabSize)\n",
    "    return pattern\n",
    "\n",
    "intToChar = dict((i, char) for i, char in enumerate(uniqueChars))  #creating a demapping of our original encoding\n",
    "seed = generateSeedFromData(data) #get a random starting point from our paper and let the network continue the writing\n",
    "print(\"Starting Seed: \", ''.join([intToChar[value] for value in seed]), end= '\\n\\n\\n')\n",
    "numCharacters= 200   #length of each window the network will use to predict the output\n",
    "text = generateText(model, seed, intToChar, length= numCharacters, vocabSize= numUniqueChars)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3066\n"
     ]
    }
   ],
   "source": [
    "##Let's look at a list of all unique characters in our scripts, \n",
    "##we'll eventually need to one hot encode them to make training easier:\n",
    "uniqueWords = sorted(transcript.split(' '))\n",
    "numUniqueWords = len(uniqueWords)\n",
    "\n",
    "##Lets make a mapping of each character to a specific number, this will help our training since we need numerical data:\n",
    "stringToInt = dict((string, i) for i, string in enumerate(uniqueWords))\n",
    "print(numUniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengthOfSequence = 25\n",
    "words = transcript.split(\" \")\n",
    "data, targets = prepSequences(words, stringToInt, lengthOfSequence)\n",
    "preppedX = prepX(data, lengthOfSequence, numUniqueWords)\n",
    "preppedY = prepY(targets)\n",
    "#print([intToString[point] for point in data[10]], intToString[targets[10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = generateModel(preppedX, preppedY, size= 512)\n",
    "model = trainModel(model, preppedX, preppedY, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'weights-improvement-05-6.2815.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-424b8ef28310>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"weights-improvement-05-6.2815.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mintToString\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniqueWords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#creating a demapping of our original encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerateSeedFromData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#get a random starting point from our paper and let the network continue the writing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnumWords\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m   \u001b[0;31m#length of each window the network will use to predict the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-3c8b37465a95>\u001b[0m in \u001b[0;36mloadModel\u001b[0;34m(model, filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##The code below loads back in the best weights we add\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#replace with best weights file for your training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`load_weights` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'weights-improvement-05-6.2815.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "model = loadModel(model, \"weights-improvement-05-6.2815.hdf5\")\n",
    "intToString = dict((i, word) for i, word in enumerate(uniqueWords))  #creating a demapping of our original encoding\n",
    "seed = generateSeedFromData(data) #get a random starting point from our paper and let the network continue the writing\n",
    "numWords= 100   #length of each window the network will use to predict the output\n",
    "text = generateText(model, seed, intToString, length= numWords, vocabSize= numUniqueWords, delimeter= ' ')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
