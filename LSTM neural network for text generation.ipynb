{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "##First we'll import all our tools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17409\n"
     ]
    }
   ],
   "source": [
    "##Let's get our scripts ready to go, we'll read in all the data\n",
    "def getData(csvName= 'transcripts.csv'):\n",
    "    dataFrame = pd.read_csv(csvName)\n",
    "    allScripts = dataFrame['transcript'].tolist()\n",
    "    allScripts = [script.lower() for script in allScripts]\n",
    "    return allScripts\n",
    "\n",
    "\n",
    "useAll = False #Set to true to use all transcripts in your training data, false uses the first transcript\n",
    "\n",
    "allScripts = getData()\n",
    "transcript = ''\n",
    "\n",
    "if not useAll:\n",
    "    transcript = allScripts[0]\n",
    "else:\n",
    "    transcript = \"\\n\".join(allScripts) #Joining our transcripts seperated by new lines\n",
    "\n",
    "print(len(transcript))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '5', '6', '9', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—']\n"
     ]
    }
   ],
   "source": [
    "##Let's look at a list of all unique characters in our scripts, \n",
    "##we'll eventually need to one hot encode them to make training easier:\n",
    "uniqueChars = sorted(list(set(transcript)))\n",
    "numUniqueChars = len(chars)\n",
    "print(uniqueChars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t': 38, '\"': 2, '?': 18, 'g': 25, \"'\": 3, '-': 7, 'q': 35, '9': 15, '.': 8, 'p': 34, 'd': 22, 'x': 42, 'o': 33, 's': 37, 'w': 41, 'k': 29, 'n': 32, 'i': 27, 'v': 40, ';': 17, 'm': 31, 'h': 26, '2': 11, '1': 10, ',': 6, 'e': 23, ')': 5, 'r': 36, 'z': 44, ' ': 0, '!': 1, '3': 12, '—': 45, 'b': 20, ':': 16, '(': 4, 'j': 28, '5': 13, '0': 9, 'u': 39, '6': 14, 'c': 21, 'l': 30, 'y': 43, 'f': 24, 'a': 19}\n"
     ]
    }
   ],
   "source": [
    "##Lets make a mapping of each character to a specific number, this will help our training since we need numerical data:\n",
    "charsToInt = dict((char, i) for i, char in enumerate(uniqueChars))\n",
    "print(charsToInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 33, 33, 22, 0, 31, 33, 36, 32, 27, 32, 25, 8, 0, 26, 33, 41, 0, 19, 36, 23, 0, 43, 33, 39, 18, 4, 30, 19, 39, 25, 26, 38, 23, 36, 5, 27, 38, 3, 37, 0, 20, 23, 23, 32, 0, 25, 36, 23, 19, 38, 6, 0, 26, 19, 37, 32, 3, 38, 0, 27, 38, 18, 0, 27, 3, 40, 23, 0, 20, 23, 23, 32, 0, 20, 30, 33, 41, 32, 0, 19, 41, 19, 43, 0, 20, 43, 0, 38, 26, 23, 0, 41, 26, 33, 30, 23, 0, 38, 26]\n"
     ]
    }
   ],
   "source": [
    "##For our network we are going to train it by feeding it strings of characters and have it predict what the next\n",
    "##character in the sequence will be. So to generate these sequences we will copy 100 characters from our transcript\n",
    "##into a sequence, take the next character as the target answer, and then shift our window by one character and do that\n",
    "##over and over again. \n",
    "\n",
    "lengthOfSequence = 100\n",
    "def prepSequences(rawText, encoding, sequenceLength = 100): \n",
    "    data = []\n",
    "    targets = []\n",
    "    for i in range(0, len(rawText) - sequenceLength, 1):\n",
    "        sequence = rawText[i: i+sequenceLength]\n",
    "        target = rawText[i + sequenceLength]\n",
    "        data.append([encoding[char] for char in sequence]) #Here we are encoding the characters to their previous assigned values\n",
    "        targets.append(encoding[target])                   #Same with the target answer\n",
    "\n",
    "    return data, targets\n",
    "\n",
    "data, targets = prepSequences(transcript, charsToInt, lengthOfSequence)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "##To finish off prepping our data, we need to convert x to be [samples, time steps, features]\n",
    "##and we need to convert our training answers to a one hot encoding\n",
    "def prepX(data, lengthOfSequence, numUniqueChars):\n",
    "    data = np.reshape(data, (len(data), lengthOfSequence, 1))\n",
    "    data = data / float(numUniqueChars)\n",
    "    return data\n",
    "\n",
    "def prepY(targets):\n",
    "    targets = np_utils.to_categorical(targets)\n",
    "    return targets\n",
    "\n",
    "preppedX = prepX(data, lengthOfSequence, numUniqueChars)\n",
    "preppedY = prepY(targets)\n",
    "\n",
    "\n",
    "##The last thing we can do before we train is get our model set up\n",
    "def generateModel(X, y):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y.shape[1], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "model = generateModel(preppedX, preppedY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "17309/17309 [==============================] - 127s 7ms/step - loss: 2.9788\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.97876, saving model to weights-improvement-01-2.9788.hdf5\n",
      "Epoch 2/5\n",
      "17309/17309 [==============================] - 129s 7ms/step - loss: 2.8639\n",
      "\n",
      "Epoch 00002: loss improved from 2.97876 to 2.86389, saving model to weights-improvement-02-2.8639.hdf5\n",
      "Epoch 3/5\n",
      "17309/17309 [==============================] - 131s 8ms/step - loss: 2.7983\n",
      "\n",
      "Epoch 00003: loss improved from 2.86389 to 2.79827, saving model to weights-improvement-03-2.7983.hdf5\n",
      "Epoch 4/5\n",
      "17309/17309 [==============================] - 141s 8ms/step - loss: 2.7374\n",
      "\n",
      "Epoch 00004: loss improved from 2.79827 to 2.73744, saving model to weights-improvement-04-2.7374.hdf5\n",
      "Epoch 5/5\n",
      "17309/17309 [==============================] - 149s 9ms/step - loss: 2.6795\n",
      "\n",
      "Epoch 00005: loss improved from 2.73744 to 2.67949, saving model to weights-improvement-05-2.6795.hdf5\n"
     ]
    }
   ],
   "source": [
    "##Training time!\n",
    "\n",
    "def trainModel(model, X, y, numEpochs= 20, batchSize= 128):\n",
    "    filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\" #replace with lowest loss file\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    model.fit(X, y, epochs = numEpochs, batch_size= batchSize, callbacks=callbacks_list)\n",
    "    return model\n",
    "\n",
    "model = trainModel(model, preppedX, preppedY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "##The code below loads back in the best weights we add\n",
    "filename = \"weights-improvement-04-2.7374.hdf5\" #replace with best weights file for your training\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Seed:  e is wonderfully interactive. the brain isn't divided into compartments. in fact, creativity — which\n",
      "\n",
      "\n",
      "e is wonderfully interactive. the brain isn't divided into compartments. in fact, creativity — which  and the the the the the the the the the the the the the the the the the the the the the the the th\n"
     ]
    }
   ],
   "source": [
    "##Now for text generation\n",
    "def generateSeedFromData(data):\n",
    "    start = np.random.randint(0, len(data)-1)\n",
    "    pattern = data[start]\n",
    "    print(\"Starting Seed: \", ''.join([intToChar[value] for value in pattern]), end= '\\n\\n\\n')\n",
    "    return pattern\n",
    "\n",
    "def generateText(model, pattern, decoding, length= 1000, vocabSize= 47):\n",
    "    text= ''.join([decoding[value] for value in pattern])\n",
    "    for i in range(length):\n",
    "        preppedPattern = prepSeed(pattern, vocabSize)\n",
    "        prediction = model.predict(preppedPattern, verbose= 0)\n",
    "        index = np.argmax(prediction)\n",
    "        result = decoding[index]\n",
    "        text += result\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:]\n",
    "        \n",
    "    return text\n",
    "\n",
    "\n",
    "def prepPattern(pattern, vocabSize):\n",
    "    pattern = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    pattern = pattern / float(vocabSize)\n",
    "    return pattern\n",
    "\n",
    "intToChar = dict((i, char) for i, char in enumerate(chars))  #creating a demapping of our original encoding\n",
    "seed = generateSeedFromData(data) #get a random starting point from our paper and let the network continue the writing\n",
    "numCharacters= 100   #length of each window the network will use to predict the output\n",
    "text = generateText(model, seed, intToChar, length= numCharacters, vocabSize= numUniqueChars)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
