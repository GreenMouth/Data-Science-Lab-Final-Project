{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/corbosiny/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##First we'll import all our tools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17409\n"
     ]
    }
   ],
   "source": [
    "##Let's get our scripts ready to go, we'll read in all the \n",
    "def getData(csvName= 'transcripts.csv'):\n",
    "    dataFrame = pd.read_csv(csvName)\n",
    "    allScripts = dataFrame['transcript'].tolist()\n",
    "    allScripts = [script.lower() for script in allScripts]\n",
    "    return allScripts\n",
    "\n",
    "\n",
    "useAll = False #Set to true to use all transcripts in your training data, false uses the first transcript\n",
    "\n",
    "allScripts = getData()\n",
    "transcript = ''\n",
    "\n",
    "if not useAll:\n",
    "    transcript = allScripts[0]\n",
    "else:\n",
    "    transcript = \"\\n\".join(allScripts) #Joining our transcripts seperated by new lines\n",
    "\n",
    "print(len(transcript))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '5', '6', '9', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—']\n"
     ]
    }
   ],
   "source": [
    "##Let's look at a list of all unique characters in our scripts, \n",
    "##we'll eventually need to one hot encode them to make training easier:\n",
    "uniqueChars = sorted(list(set(transcript)))\n",
    "numUniqueChars = len(uniqueChars)\n",
    "print(uniqueChars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l': 30, '5': 13, 'b': 20, 'i': 27, '?': 18, 'c': 21, ',': 6, '(': 4, 's': 37, 'v': 40, 'a': 19, '!': 1, 'h': 26, '—': 45, ' ': 0, 'm': 31, '-': 7, '3': 12, 'w': 41, '2': 11, 'o': 33, '\"': 2, 'd': 22, 'p': 34, 'k': 29, 'u': 39, ')': 5, 'x': 42, '6': 14, 'z': 44, 'y': 43, '1': 10, 'g': 25, 'f': 24, 'q': 35, 'j': 28, 't': 38, 'n': 32, '.': 8, 'e': 23, ';': 17, ':': 16, '9': 15, '0': 9, 'r': 36, \"'\": 3}\n"
     ]
    }
   ],
   "source": [
    "##Lets make a mapping of each character to a specific number, this will help our training since we need numerical data:\n",
    "charsToInt = dict((char, i) for i, char in enumerate(uniqueChars))\n",
    "print(charsToInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 33, 33, 22, 0, 31, 33, 36, 32, 27, 32, 25, 8, 0, 26, 33, 41, 0, 19, 36, 23, 0, 43, 33, 39, 18, 4, 30, 19, 39, 25, 26, 38, 23, 36, 5, 27, 38, 3, 37, 0, 20, 23, 23, 32, 0, 25, 36, 23, 19, 38, 6, 0, 26, 19, 37, 32, 3, 38, 0, 27, 38, 18, 0, 27, 3, 40, 23, 0, 20, 23, 23, 32, 0, 20, 30, 33, 41, 32, 0, 19, 41, 19, 43, 0, 20, 43, 0, 38, 26, 23, 0, 41, 26, 33, 30, 23, 0, 38, 26]\n"
     ]
    }
   ],
   "source": [
    "##For our network we are going to train it by feeding it strings of characters and have it predict what the next\n",
    "##character in the sequence will be. So to generate these sequences we will copy 100 characters from our transcript\n",
    "##into a sequence, take the next character as the target answer, and then shift our window by one character and do that\n",
    "##over and over again. \n",
    "\n",
    "lengthOfSequence = 100\n",
    "def prepSequences(rawText, encoding, sequenceLength = 100): \n",
    "    data = []\n",
    "    targets = []\n",
    "    for i in range(0, len(rawText) - sequenceLength, 1):\n",
    "        sequence = rawText[i: i+sequenceLength]\n",
    "        target = rawText[i + sequenceLength]\n",
    "        data.append([encoding[char] for char in sequence]) #Here we are encoding the characters to their previous assigned values\n",
    "        targets.append(encoding[target])                   #Same with the target answer\n",
    "\n",
    "    return data, targets\n",
    "\n",
    "data, targets = prepSequences(transcript, charsToInt, lengthOfSequence)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##To finish off prepping our data, we need to convert x to be [samples, time steps, features]\n",
    "##and we need to convert our training answers to a one hot encoding\n",
    "def prepX(data, lengthOfSequence, numUniqueChars):\n",
    "    data = np.reshape(data, (len(data), lengthOfSequence, 1))\n",
    "    data = data / float(numUniqueChars)\n",
    "    return data\n",
    "\n",
    "def prepY(targets):\n",
    "    targets = np_utils.to_categorical(targets)\n",
    "    return targets\n",
    "\n",
    "preppedX = prepX(data, lengthOfSequence, numUniqueChars)\n",
    "preppedY = prepY(targets)\n",
    "\n",
    "\n",
    "##The last thing we can do before we train is get our model set up\n",
    "def generateModel(X, y):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y.shape[1], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "model = generateModel(preppedX, preppedY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "3041/3041 [==============================] - 7s 2ms/step - loss: 7.1352\n",
      "\n",
      "Epoch 00001: loss improved from inf to 7.13517, saving model to weights-improvement-01-7.1352.hdf5\n"
     ]
    }
   ],
   "source": [
    "##Training time!\n",
    "\n",
    "def trainModel(model, X, y, numEpochs= 20, batchSize= 128):\n",
    "    filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\" #replace with lowest loss file\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    model.fit(X, y, epochs = numEpochs, batch_size= batchSize, callbacks=callbacks_list)\n",
    "    return model\n",
    "\n",
    "model = trainModel(model, preppedX, preppedY, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "##The code below loads back in the best weights we add\n",
    "def loadModel(model, filename):  #replace with best weights file for your training\n",
    "    model.load_weights(filename)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "model = loadModel(model, \"weights-improvement-01-3.0578.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1278",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-aa547f487c97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mintToChar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniqueChars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#creating a demapping of our original encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerateSeedFromData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#get a random starting point from our paper and let the network continue the writing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting Seed: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mintToChar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'\\n\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mnumCharacters\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m   \u001b[0;31m#length of each window the network will use to predict the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerateText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintToChar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnumCharacters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabSize\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnumUniqueChars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-aa547f487c97>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mintToChar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniqueChars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#creating a demapping of our original encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerateSeedFromData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#get a random starting point from our paper and let the network continue the writing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting Seed: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mintToChar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'\\n\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mnumCharacters\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m   \u001b[0;31m#length of each window the network will use to predict the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerateText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintToChar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnumCharacters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabSize\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnumUniqueChars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1278"
     ]
    }
   ],
   "source": [
    "##Now for text generation\n",
    "def generateSeedFromData(data):\n",
    "    start = np.random.randint(0, len(data)-1)\n",
    "    pattern = data[start]\n",
    "    return pattern\n",
    "\n",
    "\n",
    "\n",
    "def generateText(model, pattern, decoding, length= 1000, vocabSize= 47, delimeter= ''):\n",
    "    text= delimeter.join([decoding[value] for value in pattern])\n",
    "    for i in range(length):\n",
    "        preppedPattern = prepPattern(pattern, vocabSize)\n",
    "        prediction = model.predict(preppedPattern, verbose= 0)\n",
    "        index = np.argmax(prediction)\n",
    "        result = decoding[index]\n",
    "        text += delimeter + result\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:]\n",
    "        \n",
    "    return text\n",
    "\n",
    "def prepPattern(pattern, vocabSize):\n",
    "    pattern = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    pattern = pattern / float(vocabSize)\n",
    "    return pattern\n",
    "\n",
    "intToChar = dict((i, char) for i, char in enumerate(uniqueChars))  #creating a demapping of our original encoding\n",
    "seed = generateSeedFromData(data) #get a random starting point from our paper and let the network continue the writing\n",
    "print(\"Starting Seed: \", ''.join([intToChar[value] for value in seed]), end= '\\n\\n\\n')\n",
    "numCharacters= 200   #length of each window the network will use to predict the output\n",
    "text = generateText(model, seed, intToChar, length= numCharacters, vocabSize= numUniqueChars)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Let's look at a list of all unique characters in our scripts, \n",
    "##we'll eventually need to one hot encode them to make training easier:\n",
    "uniqueWords = sorted(transcript.split(' '))\n",
    "numUniqueWords = len(uniqueWords)\n",
    "\n",
    "##Lets make a mapping of each character to a specific number, this will help our training since we need numerical data:\n",
    "stringToInt = dict((string, i) for i, string in enumerate(uniqueWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['been', 'blown', 'away', 'by', 'the', 'whole', 'thing.', 'in', 'fact,', \"i'm\", 'leaving.(laughter)there', 'have', 'been', 'three', 'themes', 'running', 'through', 'the', 'conference', 'which', 'are', 'relevant', 'to', 'what', 'i'] want\n"
     ]
    }
   ],
   "source": [
    "lengthOfSequence = 25\n",
    "words = transcript.split(\" \")\n",
    "data, targets = prepSequences(words, stringToInt, lengthOfSequence)\n",
    "preppedX = prepX(data, lengthOfSequence, numUniqueWords)\n",
    "preppedY = prepY(targets)\n",
    "print([intToString[point] for point in data[10]], intToString[targets[10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3041/3041 [==============================] - 34s 11ms/step - loss: 7.2040\n",
      "\n",
      "Epoch 00001: loss improved from inf to 7.20399, saving model to weights-improvement-01-7.2040.hdf5\n",
      "Epoch 2/5\n",
      "3041/3041 [==============================] - 32s 10ms/step - loss: 6.6807\n",
      "\n",
      "Epoch 00002: loss improved from 7.20399 to 6.68073, saving model to weights-improvement-02-6.6807.hdf5\n",
      "Epoch 3/5\n",
      "3041/3041 [==============================] - 32s 11ms/step - loss: 6.5085\n",
      "\n",
      "Epoch 00003: loss improved from 6.68073 to 6.50845, saving model to weights-improvement-03-6.5085.hdf5\n",
      "Epoch 4/5\n",
      "3041/3041 [==============================] - 32s 11ms/step - loss: 6.3964\n",
      "\n",
      "Epoch 00004: loss improved from 6.50845 to 6.39644, saving model to weights-improvement-04-6.3964.hdf5\n",
      "Epoch 5/5\n",
      "3041/3041 [==============================] - 32s 11ms/step - loss: 6.3664\n",
      "\n",
      "Epoch 00005: loss improved from 6.39644 to 6.36638, saving model to weights-improvement-05-6.3664.hdf5\n"
     ]
    }
   ],
   "source": [
    "model = generateModel(preppedX, preppedY)\n",
    "model = trainModel(model, preppedX, preppedY, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conference which are relevant to what i want to talk about. one is the extraordinary evidence of human creativity in all of the presentations that the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = loadModel(model, \"weights-improvement-05-6.3664.hdf5\")\n",
    "intToString = dict((i, word) for i, word in enumerate(uniqueWords))  #creating a demapping of our original encoding\n",
    "seed = generateSeedFromData(data) #get a random starting point from our paper and let the network continue the writing\n",
    "numWords= 100   #length of each window the network will use to predict the output\n",
    "text = generateText(model, seed, intToString, length= numWords, vocabSize= numUniqueWords, delimeter= ' ')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
